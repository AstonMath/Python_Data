{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Workshop II - Python for Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thom Harvey-Ball & Jason Laurie\n",
    "#### Aston University, February 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second Python workshop which will build on the foundations of the first session. Today, you will be working with some data to explore how Python can be used to produce descriptive statistics of data, how Python can be used to plot data, how we can perform linear regression on data using the StatsModels library and how we can plot these regressions using Seaborn.\n",
    "\n",
    "We begin by importing the libraries we will need for the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#format plots to appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will recap some of the things we did in the last session, but instead of using `numpy` we will use `pandas` and instead of `matplotlib` we will use `seaborn`. First we will import a dataset of random x and y values into a pandas DataFrame which we will call `data`. Next, we display the first few rows using the `head` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the random data to a pandas dataframe\n",
    "data = pd.read_csv('randomData.csv')\n",
    "\n",
    "#display the first 5 rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task I (10 Minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cells below to display the first 10, 20, 50 rows of the data. Then experiment to find out how to display the last few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display the first 10 rows\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display the first 20 rows\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display the first 50 rows\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display the final 10 records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen a few of the records, we should verify what type of data we are dealing with. We can do this via the `dtype` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the data type of the 'x' column\n",
    "data['x'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short task:\n",
    "Use the cell below to check the datatype of the `y` column in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the data type of the 'y' column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dtype` attribute works on columns of a dataframe as shown above. If we try to use the `dtype` attribute on a full dataframe, we get an \"attribute error\". This tells us that the object we are trying to inspect, does not have the type of metadata we are trying to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trying to use a non-existant attribute\n",
    "data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to explore a full dataframe, we need to use `dtypes` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display the datatypes of all columns in a dataframe\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes have many attributes which we can use to get a good idea of what sort of data we are looking at, how much of it there is etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II (10 Minutes):\n",
    "Use the cells below to experiment with the following dataframe attributes: `columns`, `axes`, `ndim`, `size`, `shape` and `values`. Try to work out what each attribute tells you about the data. It may be helpful for you to make some notes in the cells using comments (#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#columns:\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#axes:\n",
    "data.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ndim:\n",
    "data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#size:\n",
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shape:\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#values:\n",
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as attributes, which require no punctuation, Python also has methods which use parentheses (regardless of whether any options are specified). We have already seen two methods when we used `.head(n)` and `.tail(n)`. Both of these methods used a parameter (n) to determine how many records to display but they will still execute without the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other methods which we can use with dataframes. We will not see all of them, but a few of the more important ones are: `.describe()`, `.max()`, `.min()`, `.quantile(n)`, `.mean()`, `.std()`, `.sample(n)` and `.dropna()`. Let's explore some methods, starting with `.describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#describe our dataframe\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that `.describe()` gives a useful list of descriptive statistics for all the numeric columns of a dataframe. The next one we should look at is `.sample([n])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate a random sample of 10 records from our data and store it as a new dataframe called 'randSample'\n",
    "randSample = data.sample(10)\n",
    "\n",
    "#display the full dataframe of the random sample\n",
    "randSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it should be clear what `.randsample(n)` does. It will be useful for us when we come to use a larger dataset. Finally, we will look at `.dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it is less obvious what's happened. Actually, this time, nothing's happened! `.dropna()` removes all records from the dataframe which include any missing entries, leaving behind a dataset with better integrity. I have already ensured a complete dataset for this exercise so there were no records to remove. Again, we will need this later to ensure the data we are working on does not cause us unnecessary headaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some specific statistics from our data. If we want the mean of each column, we can use the `.mean()` method on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the mean (and other statistics) of a sample of the data. For instance, the cell below will give the mean of the final 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the cell below will calculuate only the mean of the 'x' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the next cell will calculate the standard deviation of a random sample of 10 of the last 100 records for the 'y'  column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'].tail(100).sample(10).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the next cell calculuates the lower quantile of both columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.quantile(.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that we can be more and more specific about the selection of data we want to calculate statistics for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task III (20 minutes)\n",
    "Using everything we have looked at, calculate:\\\n",
    "i) The mean of each column of the first 15 rows,\\\n",
    "ii) The standard deviation of the last 20 rows of the 'x' column,\\\n",
    "iii) The minimum value of the 'y' column in a random sample of 30 records,\\\n",
    "iv) The interquartile range of both columns in the full dataset.\\\n",
    "\\\n",
    "Use the cells below for your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The mean of the first 15 rows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stdev of 'x' in last 20 rows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#min of 'y' in random sample of 30 rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#IQR of both columns in full set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen a lot of statistics about this data, although we haven't really done much analysis yet! Personally, I think it's always worth getting a basic understanding of our data from a visual perspective before we start any analysis. Let's see how to plot data with the `seaborn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Plotting with Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we imported the `seaborn` library as `sns`. `Seaborn` has multiple types of plot which work pretty much straight out of the box. Let's experiment with a few of them now. Execute each of the cells below and see what the plot tells you about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='x', y='y', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have a line plot of the data. Seaborn plots each datapoint and links each point together. This doesn't appear to show much of a trend or correlation between 'x' and 'y'... Let's move on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(x='x', y='y', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a bar chart of the data. We are getting a rainbow of colours because Seaborn is treating each 'x' value as a different series of data and colour-coding it. You will also notice the x-axis labels are a complete mess. This is definitely not the right chart for this data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(data['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a distribution plot of the 'x' column - it is a histogram with a fitted curve. This shows that the data is pretty normally (Gaussian) distributed. We can superimpose the normal distribution with the mean and std of 'x' by using the SciPi Stats library to plot the pdf of the Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "mean = data['x'].mean()\n",
    "standard_deviation = data['x'].std()\n",
    "range = np.arange(0, 120, 0.1)\n",
    "\n",
    "sns.distplot(data['x'],ax=ax)\n",
    "sns.lineplot(range, sp.stats.norm.pdf(range, mean, standard_deviation), ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the 'x' data is somewhat Gaussian, but we don't have a very clear picture still. Let's try a scatter plot instead. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='x', y='y', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! Now we've got a much better picture of what's going on! Perhaps it's time to move onto a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: A real life example\n",
    "### Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some real data. Execute the following cell to load in a large dataset concerning the salaries of certain academics at a university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('Salaries.csv')\n",
    "salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.dtypes` and `.describe()` functions to explore the data in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#determine the datatype of each column:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#give descriptive statistics about each numeric column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "You can see that there are 78 records in total. We are going to compare the salaries of male and female academics. To do this, we need to 'slice' the data into two sets; 'female_staff' and 'male_staff'. We can do this by creating two new dataframes from conditions placed on the original larger dataframe. This has the advantage that our original data is untouched in case we need to come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split dataframe to only female academics' entries\n",
    "female_staff = salaries[salaries['sex'] == 'Female']\n",
    "female_staff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split dataframe to only male academics' entries\n",
    "male_staff = salaries[salaries['sex'] == 'Male']\n",
    "male_staff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the mean salaries of the two groups. We could just use the `.mean()` method alone, but we can also use the `print` command to present clearer data. We also need to round the statistics to a meaningful degree of accuracy; as we are talking about money, 2 decimal places is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the mean salaries of each split dataframe\n",
    "print(\"Male Average Salary: $\",round(male_staff.salary.mean(),2))\n",
    "print(\"Female Average Salary: $\",round(female_staff.salary.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the average salary for male adacdemics is greater than that for women, but it is possible that this is simply becuase the men in the university have been employed longer than the women on average. To test this we will look at an ordinary least-squares regression. We can test the correlation between length of service against salary for each sex, and determine whether the starting salary and per-year-of-service salary increase is equal for both sexes. To begin with, use the `Seaborn` package to plot a scatter graph of salary against service, colour coding for sex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot a seaborn (sns) scatterplot with service on the x axis, salary on the y axis and colour (hue) coded by sex \n",
    "#from the salaries dataframe\n",
    "ax = sns.scatterplot(x='service', y='salary', hue='sex', data=salaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have introduced a new parameter to the `scatterplot` method; 'hue' can be used to tell `Seaborn` how to separate the data and automatically produces a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to perform a linear regression on the data to see how strongly correlated an academic's salary is to the number of years of service they have, to do this we use the `statsmodels` library (smf) and the *ordinary least squares* regression function. The results of the regression are stored as a \"linear model\" which we have called `lm_salaries`.\n",
    "\n",
    "We are essentially testing the hypothesis that H_0 is \"years of service has no predictive power of salary\" and H_1 is \"years of service can predict salary\" and we are going to take a 95% confidence level. This means that we are looking for a p-value less that 0.05 when comparing our model against one which does not account for years of service.\n",
    "\n",
    "We need to pass two parameters, namely `formula` and `data`. The `formula` parameter tells the function which two variables we are correlating in the form 'dependent ~ independent'. The `data` parameter calls our original dataframe. In our case we want to see how \"salary\" depends on \"service\", so we call the function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a fitted model\n",
    "lm_salaries = smf.ols(formula=\"salary ~ service\", data=salaries).fit()\n",
    "#print model summary\n",
    "print(lm_salaries.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, we have tested the predictive capability of a person's length of employment on their salary. There are a few things that are important to us: the `R-squared` value tells us how much of the variance in salary can be attributed to service; approximately 28% of the variation in our case; the `Prob (F-statistic)` tells us how likely we are to get as good a prediction using a model that doesn't account for years of service; the `F-statistic` itself tells us how good our model is compared to an intercept only model - we now need to compare this to the critical F-value for our degrees of freedom.\n",
    "\n",
    "In this test we had 1 degree of freedom in the model and 76 dofs in the residuals, at the 95% significance level, the critical F-value is 3.966 (found using F-distribution lookup tables). As our `F-statistic` is greater than this, and our p-value (`Prob (F-statistic)`) is lower than 0.05, we can conclude that our test was significant at the 95% level and therefore we reject H_0 in favour of H_1: \"years of service *can* predict salary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, look at the parameters of the model: we have an \"intercept\" and \"service\". \"Intercept\" is an estimator for the salary of a person with no years of service, i.e newly employed. \"Service\" is essentially the gradient of the linear model and tells us how much money is added to a person's salary for each year of service. The below cell will gives these values rounded to 2dp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_salaries.params.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `seaborn` to plot the regression too. The cell below plots the regression of `salaries['service']` against `salaries['salary']` at the 95% `ci` (confidence interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(salaries[['service']],salaries['salary'],ci=95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial results tell us that service does influence salary so we need to see if that accounts for the difference in the male/female salary averages. We can start with a t-test of the hypothesis that the average salary for men and women is the same. We are now testing (at the 95% confidence level) that H_0: \"male average salary = female average salary\" and H_1: \"male average salary does not equal female average salary\". Note that this is a two-tailed test (the default in Python) and we are looking for a `pvalue` less than 0.05 if we want to reject the null hypothesis.\n",
    "\n",
    "Use the `SciPy` statistics package to compare the means of the salaries of the split dataframes `male_staff` and `female_staff`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t-test comparing average salaries for men and women\n",
    "sp.stats.ttest_ind(male_staff.salary, female_staff.salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the p-value of the t-statistic is 0.027 which is less than 0.05. We can therefore reject the hypothesis that the two averages are the same.\n",
    "\n",
    "Let's do another regression but this time split by sex, this will give us an estimate at the 95% significance level for the starting salaries and yearly increases for men and women seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lm_salaries_male = smf.ols(formula=\"salary ~ service\", data=male_staff).fit()\n",
    "lm_salaries_female = smf.ols(formula=\"salary ~ service\", data=female_staff).fit()\n",
    "\n",
    "print('Male Results:', lm_salaries_male.summary())\n",
    "print('Female Results:', lm_salaries_female.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases above, the p-values are less than 0.05 so there is a definite predictive power of length of service on the two sets of salaries. Let's look at the parameters of the two models together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_salaries_male.params.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the predicted starting salary for a male academic with no service is \\\\$96792.52 and the yearly salary increase is \\\\$984.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_salaries_female.params.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the predicted starting salary for a female academic with no service is \\\\$82068.51 and the yearly salary increase is \\\\$1637.30\n",
    "\n",
    "This regression suggests that female academics start on a lower salary than their male counterparts, but receive much higher yearly increases to their salaries.\n",
    "\n",
    "We can plot the two regressions together using `Seaborn's` 'regplot' as before. As we are superimposing two regressions onto the same axis, we need to explicitly colour code rather than using \"hue\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x='service', y='salary', data=male_staff, ax=ax, color='blue');\n",
    "sns.regplot(x='service', y='salary', data=female_staff, ax=ax, color='orange');\n",
    "ax.legend(('Male','Female'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the starting salary for women appears to be much lower than that for men, but once employed they gain salary increases much faster based on their years of employment. We need to be careful with this conclusion however; there is little data for female salaries after 30 years of service and it may be that the yearly progression slows down again. \n",
    "\n",
    "We have more data for male academics at the post-30 years mark which makes the regression more reliable. Notice that there is also an outlier in the data - a male academic with over 50 years' service but a salary of only approx. \\\\$60000 - we should remove this datapoint if we want a more accurate picture.\n",
    "\n",
    "We can calculate a \"z-score\" for each datapoint in the dataframe. The z-score tells us how many standard deviations a datapoint is from the mean. If it is too far away, we can remove it as an outlier.\n",
    "\n",
    "First, we calculuate the z-scores for each data_set and display the location of any datapoints which are more than 2 standard deviations away from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of z-scores\n",
    "z_male = np.abs(sp.stats.zscore(male_staff[['salary','service']]))\n",
    "\n",
    "#print the array locations of datapoints which are too far away from the mean\n",
    "print(np.where(z_male>2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a little hard to interpret, but the first array [0, 0, 9] tells us that there is an outlier in the 9th row, the second array [0,1,1] tells us it is an outlier in both the second ('salary') and third ('service') columns.\n",
    "\n",
    "We can create a new dataframe with all the records of male_staff which are within 2 standard deviations of the mean; this should remove any outliers from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe column-by-column where all datapoints are within 2stds of the mean\n",
    "male_staff_no_outliers = male_staff[(z_male < 2).all(axis=1)]\n",
    "male_staff_no_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Task\n",
    "We should also do the same for the female data. Use the cell below to remove the outliers from the female data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of z-scores called z_female\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print the array locations of datapoints which are too far away from the mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create a new dataframe called female_staff_no_outliers column-by-column \n",
    "#where all datapoints are within 2stds of the mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the same regression but using the data without outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x='service', y='salary', data=male_staff_no_outliers, ax=ax, color='blue');\n",
    "sns.regplot(x='service', y='salary', data=female_staff_no_outliers, ax=ax, color='orange');\n",
    "ax.legend(('Male','Female'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trends are closer together and that the length of time for a female academic to catch up to her male counterparts is actually longer than our original regression suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "Complete another regression analysis to see how the number of PhD supervisions affects a person's salary, then compare male and female PhD supervisions seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot a seaborn (sns) scatterplot with phd on the x axis, salary on the y axis and colour (hue) coded by sex \n",
    "#from the salaries dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a fitted model called lm_salaries_phd using the formula \"salary~phd\"\n",
    "\n",
    "\n",
    "\n",
    "#print model summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print the parameters of the model rounded to 2dp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot a seaborn (sns) regression plot (regplot) of 'phd' against 'salary' at the 95% significance level\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute the two seperate regressions for male and female\n",
    "#one called lm_salaries_phd_male and one called lm_salaries_phd_female both using the formula \"salary~phd\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print the two regression summaries\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print the parameters of the male regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print the parameters of the female regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the two regressions on a single axis (see example above for how to do this)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dealing with Incomplete Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when dealing with large datasets, we find that we have certain datapoints missing. Here we will see how we can transform the dataset into something which we can perform meaningful analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the dataset concerning flights across America\n",
    "flights = pd.read_csv('flights.csv')\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we have just loaded gives various observations about internal flights across America in 2013. \n",
    "\n",
    "Use the below cells to determine:\\\n",
    "i) how many airlines are included,\\\n",
    "ii) the longest departure delay,\\\n",
    "iii) the shortest flight,\\\n",
    "iv) the first and last date of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many airlines are in the dataset? (Hint: Use the unique() function on the 'carrier' column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What was the longest departure delay? (Hint: Use the max() function on the 'dep_delay' column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What was the shortest flight? (Hint: Use the min() function on the 'distance' column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What were the earliest and latest dates in the observations? (Hint: sort the data first by year, then month,\n",
    "#then day. Then read the first and last row of the sorted data)\n",
    "\n",
    "flights_chronological = flights.sort_values(by = ['year','month','day'], ascending=[True,True,True])\n",
    "\n",
    "#read the first and last row of the sorted dataframe flights_chronological\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen an example of the `sort_values` function for dataframes. From the example above, we can see that the `by` parameter defines which variables we sort by (and in which order) and then the `ascending` parameter defines which direction to sort in.\n",
    "\n",
    "We can also split the dataframe into smaller datasets. For example, the cell below isolates January's data and creates a new dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_jan = flights[flights['month']==1]\n",
    "flights_jan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (10 minutes):\n",
    "Create five new dataframes of split data:\\\n",
    "      i) All flights carried by 'UA'\\\n",
    "     ii) All flights longer than 1000 distance\\\n",
    "    iii) All flights with distination 'DCA'\\\n",
    "     iv) All flights in May with origin 'LGA'\\\n",
    "      v) All flights with departure delays longer than arrival delay (dep_delay>arr_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all UA flights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all short flights (1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all flights to DCA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all May flights from LGA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All flights with longer departure delays than arrival delays\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have spotted that some of the observations are missing (denoted as NaN). This will interfere with our statistics. The easiest way to deal with this is simply to drop all records with a missing observation from the dataframe. First let's view all records with missing observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of records with missing data\n",
    "flights_missing_data = flights[flights.isnull().any(axis=1)]\n",
    "flights_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we used `pandas` `isnull()` function. This checks for NaN values in the data. By default it counts records where *all* the observations are NaN. To modify this so that we catch all NaNs (even if it is the only one in the row), we use the `.any()` function. The parameter passed to `.any()` tells the function how to structure the new dataframe - in this case we want to preserve the indices and column names so we set `axis=1`.\n",
    "\n",
    "We can drop all these NaN records from the dataset with the `.dropna()` function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe without any NaN entries\n",
    "flights_complete = flights.dropna()\n",
    "flights_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have complete data, we can do some plotting and analysis. \n",
    "\n",
    "### Final Activity: How good an estimator is dep_delay for arr_delay?\n",
    "Start by creating a scatter plot of dep_delay against arr_delay. Colour the data by carrier. You will need `sns.scatterplot()` and to use the \"hue\" parameter and the \"data\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a scatter plot with `seaborn` plotting arr_delay against dep_delay by carrier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a strong linear relationship here. Let's finish with a linear regression of arr_delay against dep_delay to see how good a predictor of arrival delay we get from departure delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an OLS regression model called lm_flights_delays and print its summary statistics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a regression plot(sns.regplot) of 'arr_delay' against 'dep_delay' for the complete data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that you get an intercept of -7.4457 and a multiplier of 1.0138."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the exercises for today's session; both of the workshops will remain available for you to revisit. We hope you've benefitted from attending and if you have any comments/suggestions, please leave us some feedback by going to `vevox.app` and using the meeting code `135-430-077`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
